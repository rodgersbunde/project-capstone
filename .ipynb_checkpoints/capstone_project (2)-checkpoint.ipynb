{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElXDe4LP-xa3"
   },
   "source": [
    "### Project Overview\n",
    "\n",
    "Exploring the dynamics of user sentiments, behavior patterns, and recipe interactions, this project delves into the 'Recipe Reviews and User Feedback Dataset' to derive insights, develop a personalized recipe recommendation system, and enhance the culinary experience on online platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG6ohhHZ1Am9"
   },
   "source": [
    "## Background\n",
    "\n",
    "The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating.  It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYEnVD7z0ZNw"
   },
   "source": [
    "## Business Problem\n",
    "\n",
    " Bay Bistro food company wants to enhance user engagement and satisfaction on its recipe platform by providing personalized recipe recommendations to users. The existing platform has a vast collection of recipes, but users often struggle to discover new recipes that match their preferences.\n",
    " Through this Miles group has been tasked to address this challenge by developing a recommendation system that analyzes user interactions and feedback to suggest relevant recipes tailored to each user's tastes and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of4Wl_WwFpbl"
   },
   "source": [
    "### Objectives\n",
    "**Detailed Sentiment Analysis**: Utilize the star ratings and review comments to conduct a nuanced sentiment analysis, exploring the relationship between user sentiment and review attributes such as up-votes/down-votes and reply counts.\n",
    "\n",
    "**User Behavior Analysis**:\n",
    "Understand user preferences and behavior by analyzing recipe reviews, ratings, and interactions.\n",
    "Identify popular recipes and trending ingredients based on user feedback.\n",
    "\n",
    "**Personalized Recipe Recommendations:**\n",
    "Develop a recommendation algorithm to suggest recipes tailored to each user's taste and preferences.\n",
    "Utilize collaborative filtering and content-based filtering techniques to enhance personalized recommendations.\n",
    "\n",
    "**User Interface and Experience Design:**\n",
    "Develop an intuitive and user-friendly interface for users to easily browse recipes, read reviews, and receive recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM7ZH7TKgUnP"
   },
   "source": [
    "## Data Understanding\n",
    "The data set contains the following columns:\n",
    "1. recipe name: {name of the recipe the comment was posted on}\n",
    "2. recipe number: {placement of the recipe on the top 100 recipes list}\n",
    "3. recipe code: {unique id of the recipe used by the site}\n",
    "4. comment id: {unique id of the comment}\n",
    "5. user id: {unique id of the user who left the comment}\n",
    "6. user name: {name of the user}\n",
    "7. user reputation: {internal score of the site, roughly quantifying the past behavior of the user}\n",
    "8. create at: {time at which the comment was posted as a Unix timestamp}\n",
    "9. reply count: {number of replies to the comment}\n",
    "10. thumbs up: {number of up-votes the comment has received}\n",
    "11. thumbs down: {number of down-votes the comment has received}\n",
    "12. stars: {the score on a 1 to 5 scale that the user gave to the recipe. A score of 0 means that no score was given}\n",
    "13. best score: {score of the comment, likely used by the site the help determine the order in the comments that appear in}\n",
    "14. text: {the text content of the comment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQcE4pM8voLq",
    "outputId": "1e0bc919-6afa-4684-b217-b116605604c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Obtaining dependency information for surprise from https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
      "Collecting scikit-surprise (from surprise)\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "     ---------------------------------------- 0.0/772.0 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/772.0 kB ? eta -:--:--\n",
      "      ------------------------------------ 20.5/772.0 kB 330.3 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 61.4/772.0 kB 465.5 kB/s eta 0:00:02\n",
      "     ------ ----------------------------- 143.4/772.0 kB 853.3 kB/s eta 0:00:01\n",
      "     -------------- ----------------------- 286.7/772.0 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 501.8/772.0 kB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 727.0/772.0 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 772.0/772.0 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\odhiambo rodgers bon\\documents\\anaconda\\lib\\site-packages (from scikit-surprise->surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\odhiambo rodgers bon\\documents\\anaconda\\lib\\site-packages (from scikit-surprise->surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\odhiambo rodgers bon\\documents\\anaconda\\lib\\site-packages (from scikit-surprise->surprise) (1.11.1)\n",
      "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\odhiambo rodgers bon\\AppData\\Local\\Temp\\pip-install-9b726nmj\\scikit-surprise_ed61f9f92b6c4d26b93323e4dfe9f377\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\odhiambo rodgers bon\\Documents\\Anaconda\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cRx_pBcuvnCp"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split,cross_validate\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "#importing neccesary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Dataset, Reader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from surprise import KNNWithMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "vlijKoi0v5EX",
    "outputId": "e9b8c298-0c2b-41cc-cf1c-0d974db274ce"
   },
   "outputs": [],
   "source": [
    "#loading the data set\n",
    "df=pd.read_csv('Recipe.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzdFZzKuy-bz",
    "outputId": "e0bb68d2-a4ea-40b4-a806-044b8a38b05d"
   },
   "outputs": [],
   "source": [
    "#checking data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "947aG5epcRyE",
    "outputId": "ef2c2839-a481-4bf4-a23e-39fef8dddc22"
   },
   "outputs": [],
   "source": [
    "#checking data information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzXwz5CA3isK"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FBL7HP7lvhA"
   },
   "source": [
    "To ensure the development of a robust and accurate model, several data cleaning techniques will be applied to the dataset. The following techniques will be employed:\n",
    "\n",
    "- Completeness: This technique involves addressing missing values within the dataset. Steps will be taken to identify and handle missing data appropriately, either through imputation or removal, to ensure that the dataset is complete.\n",
    "\n",
    "- Consistency: The consistency of the data will be examined to identify any discrepancies or irregularities. Inconsistencies in variables, such as conflicting formats or conflicting information within the dataset, will be addressed and resolved to maintain data integrity.\n",
    "\n",
    "- Validity: Validity refers to the accuracy and relevance of the data. Data validation techniques will be applied to verify that the values within each variable align with expected ranges or predefined criteria. Any invalid or erroneous data points will be rectified or removed from the dataset.\n",
    "\n",
    "- Uniformity: Uniformity is crucial during the data cleaning process to ensure consistency and accurate analysis. Inconsistent or non-uniform data can introduce errors and bias into the modeling process, leading to unreliable results. Robust techniques will be employed to detect and handle non-uniformity effectively, ensuring that the data is standardized and aligned.\n",
    "\n",
    "By applying these data cleaning techniques, the dataset will be refined and prepared, ensuring the reliability and accuracy of the data before proceeding with the subsequent stages of analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0Ma9-YIl4Gj"
   },
   "source": [
    "### Completeness\n",
    "To achieve completeness in our data, I will be checking for missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-P7jIrOCl05J",
    "outputId": "c79b7d59-5ece-40fb-9d55-f32468ec5567"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(f'The data has {df.isna().sum().sum()} missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqY9_TMkmdKf"
   },
   "outputs": [],
   "source": [
    "# Define a function to explore missing data\n",
    "def missing_data(df):\n",
    "    missing_data = df.isna().sum()\n",
    "    missing_data = missing_data[missing_data>0]\n",
    "    return missing_data.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIpFd24zmfev"
   },
   "outputs": [],
   "source": [
    "# expanding the number of visible columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "q9CaTK7UmhyW",
    "outputId": "cca64ba6-e300-43f5-b981-8373c6f05cb0"
   },
   "outputs": [],
   "source": [
    "# Apply missing_data function to the dataframe\n",
    "missing_data(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OnkvKw-mvx8"
   },
   "outputs": [],
   "source": [
    "# Fill missing values in 'reply_count' column with 0\n",
    "df['reply_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-57TW67nxwe"
   },
   "outputs": [],
   "source": [
    "# Droping rows where the \"text\" column is missing\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XWMRyKUwsu1",
    "outputId": "c6500ce4-067f-4f9f-d842-ba2181a682fa"
   },
   "outputs": [],
   "source": [
    "# converting 'created_at' to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7nQv-Alm6nO",
    "outputId": "c23d7e6f-0520-4fd2-c196-c4d880ee5467"
   },
   "outputs": [],
   "source": [
    "# checking to see if missing values have been replaced\n",
    "print(f'The data has {df.isna().sum().sum()} missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItjK_O_GrkMU"
   },
   "source": [
    "### Consistency\n",
    "For the data to be constistent, I need to resolve any inconsistencies by checking for duplicate values in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enKQk7w7reP_",
    "outputId": "70a78b1a-10a9-4cc1-c7d4-0731414106f6"
   },
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "print(f'The data has {df.duplicated().sum()} duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cezifzuRr1U8"
   },
   "source": [
    "- The data has no duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEDQVHgmr8_i"
   },
   "source": [
    "### Validity\n",
    "\n",
    "For our data to be valid, I have to verify that every column is accurate and appropriate for this analysis and remove those that are invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "cCIPop6wsBUK",
    "outputId": "f1e4a5b6-8023-4c15-c7cf-10b6d25e2ab2"
   },
   "outputs": [],
   "source": [
    "#validity checks\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-v_QBb1ul4B"
   },
   "source": [
    "### Uniformity\n",
    "- For our data to be uniform, I have to verify that every column is correct and convert them to there appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2v7dnZvuoTi",
    "outputId": "8992b4c1-ad39-4428-8023-f6dfb44b6330"
   },
   "outputs": [],
   "source": [
    "#checking for data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5G7n1uWux2_"
   },
   "outputs": [],
   "source": [
    "#Converting data types 'reply_count' to integer\n",
    "df['reply_count'] = df['reply_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bysY4sxPkSs7"
   },
   "outputs": [],
   "source": [
    "# Convert user_id to integer values\n",
    "df['user_id'] = df['user_id'].rank(method='dense').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0kD8BndvcAc"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c8EkqQ8eVmZ"
   },
   "outputs": [],
   "source": [
    "# Rename the \"stars\" column to \"ratings\"\n",
    "df.rename(columns={'stars': 'ratings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZVjZZQDLt50"
   },
   "outputs": [],
   "source": [
    "# Extract the year from the 'created_at' column and create a new 'month' column\n",
    "df['month'] = df['created_at'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtEdQ1wxt8TL"
   },
   "outputs": [],
   "source": [
    "# # User Interaction Features\n",
    "df['thumbs_up_ratio'] = df['thumbs_up'] / (df['thumbs_up'] + df['thumbs_down'] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "CQnVKrxOuzEC",
    "outputId": "968fe34a-136a-4667-f0c2-02f559ddd09c"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3ltn6TtRApu"
   },
   "source": [
    "EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xayu9imH0ZNI"
   },
   "source": [
    "#### Visualization of rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "FtFrZ_2VeSwr",
    "outputId": "f0c58441-b81a-43c9-d2ea-0fdd93f505fb"
   },
   "outputs": [],
   "source": [
    "# Plotting with seaborn and matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['ratings'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of ratings')\n",
    "plt.xlabel('ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qkLfasI6wp7"
   },
   "source": [
    "`Observation`\n",
    "\n",
    "There seems to be a concentration of bars on the right side of the graph, suggesting a higher frequency of recipes receiving positive ratings.However, there's also a range of ratings, indicating some variation in user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqCOylAo1kX1"
   },
   "source": [
    "#### Visualizing rating trends over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "CIA25wzfvHvn",
    "outputId": "4768a7f6-1bdc-4745-b42a-b0708f29385d"
   },
   "outputs": [],
   "source": [
    "# Grouping by month to calculate average ratings\n",
    "ratings_trend = df.groupby('month')['ratings'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ratings_trend['month'], ratings_trend['ratings'], marker='o')\n",
    "plt.title('Average Ratings Trend Over Time')\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('Average Ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-mD8MdO5ZK1"
   },
   "source": [
    "`Observation`\n",
    "\n",
    "- The line shows some fluctuations in the average rating over time.There's a possibility of a slight downward trend in the average rating as the months progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH4STbjS0Nv6"
   },
   "source": [
    "### Top 5 recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "_bku7zgaRbtR",
    "outputId": "eec92618-b9bf-41f1-8625-0933384dd9c5"
   },
   "outputs": [],
   "source": [
    "# Group by recipe and count the number of stars, then sort in descending order\n",
    "top_10_recipes = df.groupby('recipe_name')['ratings'].count().sort_values(ascending=False).head()\n",
    "# list of custom colors for the bars\n",
    "custom_colors = ['#06837f', '#02cecb', '#b4ffff', '#f8e16c', '#fed811']\n",
    "# bar plot with custom colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = top_10_recipes.plot(kind='bar', color=custom_colors)\n",
    "plt.title('Top 5 Recipes')\n",
    "plt.xlabel('Recipes')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add the number of ratings on top of each bar\n",
    "for i, v in enumerate(top_10_recipes):\n",
    " ax.text(i, v, str(v), ha='center', va='bottom', fontsize=16, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHSjWXCz2fI6"
   },
   "source": [
    "### Visualization Top 10 recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "-18OpmBZ2d0K",
    "outputId": "45281f91-729a-4e39-fe95-87af2db4c63f"
   },
   "outputs": [],
   "source": [
    "# Group by recipe_name and count the number of ratings\n",
    "top_10_recipes = df['recipe_name'].value_counts().head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10_recipes.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Recipes ')\n",
    "plt.xlabel('Recipe Name')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyzF5WrRWht3"
   },
   "source": [
    " `Observation`\n",
    "\n",
    " The graph suggests that Cheeseburger Soup,Creamy White Chili,Best Ever Banana Bread,Enchilada Casserole-Ole!,Basic Homemade Bread,Favorite Chicken Potpie,Flavorful Chicken Fajitas,Amish Breakfast Casserole,Zucchini Pizza Casserole,Cauliflower Soupthese  are the most popular recipes among users based on the number of ratings they have received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu7mvTp9vVkW"
   },
   "source": [
    "#### Multivariate Analysis\n",
    "\n",
    "Our aim here is to look for the relationship between different features\n",
    "\n",
    "First we look at the correlation of the numeric columns using Pearson's coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "cIiVoPCvsEyb",
    "outputId": "9e727a00-d074-445e-fd7f-89b5a91ab6ac"
   },
   "outputs": [],
   "source": [
    "# Pearson coefficient of numeric columns\n",
    "numerical_columns_df = df[[\n",
    "    'user_reputation',\n",
    "    'reply_count',\n",
    "    'thumbs_up',\n",
    "    'thumbs_down',\n",
    "    'ratings',\n",
    "    'best_score',\n",
    "    'thumbs_up_ratio'\n",
    "]]\n",
    "numerical_columns_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEeEtIJlJW8I"
   },
   "source": [
    "`Observations`\n",
    "\n",
    " The correlation matrix provides valuable insights into the relationships between different features.\n",
    " For instance, thumbs up and best score have a strong positive correlation, suggesting that users who give more thumbs up tend to have higher best scores. On the other hand, reply count and ratings have a weak negative relationship, implying that users who reply more may give slightly lower ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0KBP4z_loBw"
   },
   "source": [
    "### MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erQcDGRRHuoT"
   },
   "source": [
    "## Item-Based Collaborative Filtering\n",
    "\n",
    "its recommendation technique that focuses on the similarity between items rather than between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlQ3L8FOwmpW"
   },
   "outputs": [],
   "source": [
    "#preparing the data by converting the provided DataFrame to a surprise dataset\n",
    "ratings_df = pd.DataFrame(df)\n",
    "\n",
    "# Define the reader object\n",
    "reader = Reader(rating_scale=(0, 100))\n",
    "\n",
    "# Load data from DataFrame\n",
    "data = Dataset.load_from_df(ratings_df[['user_id', 'recipe_code', 'ratings']], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34sie8HGOXUJ",
    "outputId": "80421244-3eea-4dd9-c679-8c07ac960b46"
   },
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Use item-based collaborative filtering\n",
    "algo_item_based = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "algo_item_based.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions_item_based = algo_item_based.test(testset)\n",
    "\n",
    "# Print RMSE (Root Mean Square Error)\n",
    "from surprise import accuracy\n",
    "print(\"Item-Based CF RMSE:\", accuracy.rmse(predictions_item_based))\n",
    "print(\"Item-Based CF MAE:\", accuracy.mae(predictions_item_based))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6h3uqfuHyhi"
   },
   "source": [
    "### User-Based Collaborative Filtering\n",
    " The recommendation technique used in information filtering systems to provide personalized recommendations to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhNH93J2nRws",
    "outputId": "663cc706-0264-49e7-f99c-b2ed9a0b93f4"
   },
   "outputs": [],
   "source": [
    "# Use user-based collaborative filtering\n",
    "algo_user_based = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "algo_user_based.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions_user_based = algo_user_based.test(testset)\n",
    "\n",
    "# Print RMSE (Root Mean Square Error)\n",
    "print(\"User-Based CF RMSE:\", accuracy.rmse(predictions_user_based))\n",
    "print(\"User-Based CF MAE:\",accuracy.mae(predictions_user_based))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFqLVzHGLb_q"
   },
   "source": [
    "## Single Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxERyP-_Bix1",
    "outputId": "41c416c6-5d12-40ab-9570-6c857cbb5895"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "# Split the dataset into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Use SVD\n",
    "model = SVD()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Print RMSE (Root Mean Square Error)\n",
    "print(\"SVD RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"SVD MAE:\", accuracy.mae(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWv8PPc0oKz3"
   },
   "source": [
    "### Recommendations based on SVD\n",
    "\n",
    "To generate recommendations using SVD, a userID to whom which recommendations are to be made is taken an input. SVD model\n",
    "is used to predicted rating for each recipe which represent how much the user might like each recipe. The ratings are sorted in a\n",
    "descending order and recommendations is given to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-aBpjteuURQ",
    "outputId": "a46f669c-73a1-41ca-98b2-a017ccf3d54a"
   },
   "outputs": [],
   "source": [
    "# Recommend movies for a specific user (user_id = 1 in this example)\n",
    "user_id = 13114\n",
    "user_recipe = df[df['user_id'] == user_id]['recipe_code'].unique()\n",
    "# Generate recommendations for the user\n",
    "recommended_recipe = []\n",
    "for recipe_code in df['recipe_code'].unique():\n",
    " if recipe_code not in user_recipe:\n",
    "  predicted_ratings = model.predict(user_id, recipe_code).est\n",
    "  recommended_recipe.append((recipe_code, predicted_ratings))\n",
    "\n",
    "# Sort recommended movies by predicted rating\n",
    "recommended_recipe.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 10 recommended recipes\n",
    "for recipe_code, predicted_rating in recommended_recipe[:10]:\n",
    " recipe_name = df[df['recipe_code'] == recipe_code]['recipe_name'].iloc[0]\n",
    " print(f\"Recipe Name: {recipe_name}, Recipe code: {recipe_code}, Predicted Rating: {predicted_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXMO2AOWmF5h"
   },
   "source": [
    "`Observation`\n",
    "\n",
    "The output represents top recommended recipes along with their predicted ratings. These recommendations  help users discover popular and potentially enjoyable recipes, enhancing user engagement and satisfaction with the recipe platform or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0vv3S2r5EKw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtEM7O9yrt0p"
   },
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGRFmIsUzjUC",
    "outputId": "80f32896-69e9-40c6-a54f-0683aaede5bd"
   },
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "# Define parameter grid\n",
    "param_grid = {'n_factors': [50, 100, 150], 'n_epochs': [20, 30, 40], 'lr_all': [0.001, 0.002, 0.005]}\n",
    "# Instantiate SVD\n",
    "model = SVD()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n",
    "grid_search.fit(data)\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params['rmse']\n",
    "print(\"Best parameters:\", best_params)\n",
    "#getting rmse score\n",
    "cv_results = grid_search.cv_results\n",
    "\n",
    "# Extract RMSEand MAE values\n",
    "rmse_values = cv_results['mean_test_rmse']\n",
    "mae_values = cv_results['mean_test_mae']\n",
    "\n",
    "# Print RMSE and MAE values\n",
    "print(\"RMSE values:\", rmse_values)\n",
    "print(\"MAE values:\", mae_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtCN1IE0rKFr",
    "outputId": "399b4dc8-b030-4ba3-bf22-473d2227a015"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the SVD model\n",
    "model = SVD(**best_params)\n",
    "trainset = data.build_full_trainset()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Get the list of all recipe codes\n",
    "all_recipe_codes = df['recipe_code'].unique()\n",
    "\n",
    "# Generate top-10 recipe recommendations for each user\n",
    "top_n = {}\n",
    "for uid in df['user_id'].unique():\n",
    "    # Exclude recipes already rated by the user\n",
    "    user_recipe_rated = df.loc[df['user_id'] == uid, 'recipe_code']\n",
    "    recipe_to_predict = [rid for rid in all_recipe_codes if rid not in user_recipe_rated]\n",
    "\n",
    "    # Predict ratings for recipes not yet rated by the user\n",
    "    predictions = [model.predict(uid, rid) for rid in recipe_to_predict]\n",
    "\n",
    "    # Sort predictions by estimated rating\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Get top 10 recipe recommendations\n",
    "    top_n[uid] = [(pred.iid, pred.est) for pred in sorted_predictions[:10]]\n",
    "\n",
    "# Print the top 10 recommendations for a specific user\n",
    "\n",
    "print(f\"Top 10 recipe recommendations for user {uid}:\")\n",
    "for rank, (recipe_code, estimated_rating) in enumerate(top_n[uid], start=1):\n",
    "    print(f\"{rank}: Recipe Code {recipe_code} (Estimated Rating: {estimated_rating})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdtnz-7UqtF5"
   },
   "source": [
    "- The estimated ratings are numerical values generated by the SVD model, representing how much the model predicts the user would like each recipe.\n",
    "- Higher estimated ratings indicate recipes that the model believes the user is more likely to enjoy or rate highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yExzFPBqsTQ-"
   },
   "source": [
    "#### Non- negative Matrix Factorization\n",
    "\n",
    "This is a matrix Factorization technique that factors the user-item interaction matrix into non-negative matrices.\n",
    "\n",
    "Non-negative Matrix Factorization (NMF) offers a parts-based, interpretable representation of the data, making it particularly useful for recommendation systems. It can capture latent features or topics that represent underlying preferences or characteristics of users and items.\n",
    "\n",
    "NMF will help enhance the performance and scalability of recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "id": "VC53HlyUsJ3g",
    "outputId": "9ff9111f-59d9-4331-b282-042d9602f794"
   },
   "outputs": [],
   "source": [
    "from surprise import NMF\n",
    "from surprise.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# NMF algorithm\n",
    "nmf = NMF()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(nmf, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "# Extract RMSE scores\n",
    "rmse_scores = cv_results['test_rmse']\n",
    "\n",
    "print(\"Cross-validation results:\", cv_results)\n",
    "\n",
    "# Plot RMSE scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 6), rmse_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Cross-Validated RMSE Scores for NMF')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw42H90K8qPG"
   },
   "source": [
    "The cross-validation results indicate the average RMSE and MAE values obtained across all folds, which are 1.5308 and 1.0460, respectively.\n",
    "\n",
    "The NMF algorithm has an average RMSE of approximately 1.5308 and an average MAE of approximately 1.0460 across the 5 folds. The model takes an average of 1.48 seconds to fit and 0.03 seconds to test on each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZIhjySO71RA"
   },
   "source": [
    "  Below we give recipe recommendation  for user_ id `13695` based on ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGSxnNyq4KN8"
   },
   "outputs": [],
   "source": [
    "# getting a list of all recipe ids\n",
    "np.random.seed(42)\n",
    "all_recipe_ids=np.unique(df['recipe_code'])\n",
    "\n",
    "# predicted ratings for all recipe in your dataset for use Id 3.\n",
    "user_id = 13695\n",
    "\n",
    "# Create a list to store predicted ratings\n",
    "predicted_ratings = []\n",
    "for recipe_code in all_recipe_ids:\n",
    " predicted_rating = nmf.predict(user_id, recipe_code).est\n",
    " predicted_ratings.append((recipe_code, predicted_rating))\n",
    "\n",
    "#sorting predicted_ratings in descending order\n",
    "predicted_ratings.sort(key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62Tyv8_t56wF",
    "outputId": "ec6b2d0f-4ebd-44c0-c6cb-f1d51ef49d66"
   },
   "outputs": [],
   "source": [
    "# getting the top 5 recommendations\n",
    "top_5_recommendations=predicted_ratings[:5]\n",
    "top_5_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoFqYf6RTO7b",
    "outputId": "26c33b41-bb3f-484c-c284-79619d1c3d09"
   },
   "outputs": [],
   "source": [
    "for recipe_code, predicted_rating in top_5_recommendations:\n",
    " recipe_title = df[df['recipe_code'] == recipe_code]['recipe_name'].values[0]\n",
    " print(f\"Recipe Name: {recipe_name}, Predicted Rating: {predicted_rating}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5wQsv2BgCPT"
   },
   "source": [
    "## Deep Learning Matrix Facrorization\n",
    "\n",
    "Matrix Factorization with Embedding Layer\n",
    "\n",
    "Matrix factorization with embeddings is a popular approach in recommendation systems to model user-item interactions. It aims to\n",
    "decompose the user-item interaction matrix into lower-dimensional embeddings for users and items (recipe in this case). These\n",
    "embeddings capture latent features that represent users' preferences and items' characteristics. By learning these embeddings, the\n",
    "model can predict how users would rate unseen items, enabling personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QD_6JTTou5zn"
   },
   "source": [
    "splitting the data into training and validation sets for recomendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV5TLgz6aVLu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 'rating'becomes the target variable 'y'\n",
    "y = df['ratings']\n",
    "# 'userId' and 'recipe_code' are feature data 'X'\n",
    "X = df[['user_id', 'recipe_code']]\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myX8BrFygrM-"
   },
   "source": [
    "calculating number of unique users and recipe to determine the dimensions of the embendding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dFYwAELasj_"
   },
   "outputs": [],
   "source": [
    "# the maximum user ID in the dataset and add 1 to account for 0-based indexing\n",
    "num_users = df['user_id'].max() + 1\n",
    "# the maximum movie ID in the dataset and add 1 to account for 0-based indexing\n",
    "num_recipe = df['recipe_code'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md1GZRGqvxgz"
   },
   "source": [
    " Neural Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVi-3QF0bETW"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Flatten,Concatenate, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define embedding dimensions\n",
    "embedding_dim = 40\n",
    "# Define input layers\n",
    "user_input = Input(shape=(1,), name='User_Input')\n",
    "recipe_input = Input(shape=(1,), name='Recipe_Input')\n",
    "\n",
    "# Define embedding layers\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='User_Embedding')(user_input)\n",
    "recipe_embedding = Embedding(input_dim=num_recipe, output_dim=embedding_dim, name='Recipe_Embedding')(recipe_input)\n",
    "\n",
    "# Flatten the embeddings\n",
    "user_flat = Flatten(name='User_Vector')(user_embedding)\n",
    "recipe_flat = Flatten(name='Recipe_Vector')(recipe_embedding)\n",
    "\n",
    "# Concatenate user and recipe embeddings\n",
    "concatenated = Concatenate(name='Concatenate')([user_flat, recipe_flat])\n",
    "# Define your model's architecture\n",
    "dense_layer = Dense(100, activation='relu', name='dense')(concatenated)\n",
    "output_layer = Dense(1, activation='linear', name='Output')(dense_layer)\n",
    "# Create the model\n",
    "model = Model(inputs=[user_input, recipe_input], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfn2brQUeUkr",
    "outputId": "a98f8ab6-ed38-44c8-b9b8-74851299887e"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo7BGJDupVve"
   },
   "source": [
    "`Observation`\n",
    "\n",
    "This model takes user and recipe IDs, converts them into embeddings, concatenates these embeddings, passes them through dense layers, and finally predicts a rating. The model is relatively large due to the high number of parameters, especially in the embedding layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7c55isVegDO"
   },
   "outputs": [],
   "source": [
    "#TensorFlow Model Compilation with Custom MAE Metric\n",
    "import tensorflow as tf\n",
    "# Custom mse metric function\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_absolute_error', mae])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZ_gEr2gfLo4",
    "outputId": "2963862b-51e4-4abe-8ff9-48f3d21e1dfb"
   },
   "outputs": [],
   "source": [
    "# Training the Model on User and Recipe_code with Ratings\n",
    "history = model.fit(\n",
    " [X_train['user_id'].values, X_train['recipe_code'].values], # Input data for User and recipe_code\n",
    " y_train , # Target values (ratings)\n",
    " epochs=20, # Number of training epochs\n",
    " batch_size=128, # Batch size for training\n",
    " validation_data=([X_val['user_id'].values, X_val['recipe_code'].values], y_val) # Validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "bxVz_n7GhH28",
    "outputId": "b2615a40-be97-4660-df9e-26ccadd73b5b"
   },
   "outputs": [],
   "source": [
    "# training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "# count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "# Visualize loss history\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epoch_count, training_loss, 'r--', label='Training Loss')\n",
    "plt.plot(epoch_count, test_loss, 'b-', label='Test Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmywHhdLYgwD"
   },
   "source": [
    "The graph suggests that there's no simple relationship between a recipe's complexity and its average rating in this dataset. Complex recipes can be highly rated, and simple recipes can also be well-received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuiF3l3DXEJk"
   },
   "source": [
    "Generating Top Recipe Recommendations for a User `13695 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zBZcUknRxnS",
    "outputId": "a7e1c179-3df7-4f62-9ec3-e944e58f58b4"
   },
   "outputs": [],
   "source": [
    "#Generating Top Recipe Recommendations for a User `13695 `\n",
    "user_id = 13695\n",
    "all_recipe_ids = np.unique(ratings_df['recipe_code'])\n",
    "\n",
    "predicted_ratings = []\n",
    "\n",
    "for recipe_code in all_recipe_ids:\n",
    "    # Predict the rating for the user and recipe\n",
    "    predicted_rating = model.predict([np.array([user_id]), np.array([recipe_code])])[0][0]\n",
    "\n",
    "\n",
    "    # Clip the predicted rating to the range of 0.5 to 5.0\n",
    "    predicted_rating = max(0.5, min(predicted_rating, 5.0))\n",
    "\n",
    "    predicted_ratings.append((recipe_code, predicted_rating))\n",
    "\n",
    "# Sort the predicted ratings to find top recommendations\n",
    "predicted_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "top_recommendations = predicted_ratings[:10]\n",
    "\n",
    "# Print top recommendations\n",
    "print(\"Top 10 Recommended Recipes for User\", user_id)\n",
    "\n",
    "for i, (recipe_code, predicted_rating) in enumerate(top_recommendations):\n",
    "    recipe_name = df.loc[df['recipe_code'] == recipe_code, 'recipe_name'].iloc[0]\n",
    "    print(f\"Top {i+1}: {recipe_name} (Predicted Rating: {predicted_rating})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AivXcv5YBj7"
   },
   "source": [
    "The recommendations are based on the predicted ratings generated by the recommendation model for the  user. The predicted ratings are sorted in descending order to identify the top 10 recipes that the user is most likely to enjoy. Recipes with higher predicted ratings are considered to be more aligned with the user's preferences, while those with lower ratings may be less preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Save top recommendations as a pickle file\n",
    "with open('top_recommendations.pkl', 'wb') as f:\n",
    "    pickle.dump(top_recommendations, f)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
